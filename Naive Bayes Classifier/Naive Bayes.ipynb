{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Naive Bayes Classifier",
   "id": "6b11e76104b629f"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Import Libraries:",
   "id": "971b54deb4a75ffc"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-23T23:48:57.395266Z",
     "start_time": "2026-01-23T23:48:47.048123Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "from util import get_data\n",
    "from datetime import datetime\n",
    "from scipy.stats import norm\n",
    "from scipy.stats import multivariate_normal as mvn"
   ],
   "id": "fda638f26fe326e2",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Implementation, mathematics:\n",
    "##### We assume independence between features (the Naive Bayes case). The Formula (Independent Features). When features are independent, the joint probability $p(x)$ is simply the product of individual probabilities for each feature.  The formula becomes:$$p(x | \\mu, \\sigma^2) = \\prod_{i=1}^{d} \\frac{1}{\\sqrt{2\\pi\\sigma_i^2}} \\exp\\left( -\\frac{(x_i - \\mu_i)^2}{2\\sigma_i^2} \\right)$$Where:\n",
    "* $d$ is the number of features.\n",
    "* $\\mu_i$ is the mean of the $i$-th feature.\n",
    "* $\\sigma_i^2$ is the variance of the $i$-th feature.\n",
    "\n",
    "##### Why the Covariance Matrix Changes?\n",
    "\n",
    "In a general (Non-Naive) Gaussian model, we use a full covariance matrix $\\Sigma$. However, under the independence assumption:\n",
    "1) Diagonal Matrix:  The covariance between different features $i$ and $j$ ($i \\neq j$) is $0$.\n",
    "2) Structure of $\\Sigma$: The matrix $\\Sigma$ becomes a diagonal matrix, where only the variances ($\\sigma^2$) stay on the main diagonal.\n",
    "\n",
    "$$\\Sigma = \\begin{bmatrix} \\sigma_1^2 & 0 & \\dots & 0 \\\\ 0 & \\sigma_2^2 & \\dots & 0 \\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\ 0 & 0 & \\dots & \\sigma_d^2 \\end{bmatrix}$$"
   ],
   "id": "7c382013045f726b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-23T23:48:57.410430Z",
     "start_time": "2026-01-23T23:48:57.401858Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class NaiveBayes(object):\n",
    "    def fit(self, X, Y, smoothing = 10e-3):\n",
    "\n",
    "        # Creating empty dictionaries\n",
    "        self.gaussians = dict()\n",
    "            # self.gaussians = {0:{\"mean\": 3.41, 4.42, 5.23, \"variance\": 1.34, 1.23, 1.53}, 1:mean\": ..., \"variance\": ...}}\n",
    "\n",
    "        self.priors = dict()\n",
    "            # self.priors = {0:p(Y = 0), 1:p(Y = 1)}\n",
    "\n",
    "        # Unique values of Y\n",
    "        labels = set(Y)\n",
    "\n",
    "        # Filling in data for our empty dictionaries\n",
    "        for c in labels:\n",
    "            current_x = X[Y == c]\n",
    "            self.gaussians[c] = {\"mean\" : current_x.mean(axis = 0), \"variance\" : current_x.var(axis = 0) + smoothing}\n",
    "            # P(Y) - Prior\n",
    "            self.priors[c] = float(len(Y[Y == c]) / len(Y))\n",
    "\n",
    "    def predict(self, X):\n",
    "        N, D = X.shape\n",
    "        K = len(self.gaussians)\n",
    "        P = np.zeros((N, K))\n",
    "        for c, g in self.gaussians.iteritems():\n",
    "            mean, variance = g[\"mean\"], g[\"variance\"]\n",
    "            # P(X|Y) - Likelihood MVN\n",
    "            P[:, c] = mvn.logpdf(X, mean=mean, variance=variance) + np.log(self.priors[c])\n",
    "        return np.argmax(P, axis = 1)\n",
    "\n",
    "    def score(self, X, Y):\n",
    "        P = self.predict(X)\n",
    "        return np.mean(P == Y)"
   ],
   "id": "8311b79d32a132bb",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Load data, run all code",
   "id": "6044ba64c5c48f42"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-24T18:12:04.790884Z",
     "start_time": "2026-01-24T18:12:04.786893Z"
    }
   },
   "cell_type": "code",
   "source": [
    "if __name__ == '__main__':\n",
    "    X, Y = get_data(10000)\n",
    "    Ntrain = len(Y) / 2\n",
    "    Xtrain, Ytrain = X[:Ntrain], Y[:Ntrain]\n",
    "    Xtest, Ytest = X[Ntrain:], Y[Ntrain:]\n",
    "\n",
    "    model = NaiveBayes()\n",
    "    t0 = datetime.now()\n",
    "    model.fit(Xtrain, Ytrain)\n",
    "    print(\"Training time: \", datetime.now() - t0)\n",
    "\n",
    "    t0 = datetime.now()\n",
    "    print(\"Test accuracy:\", model.score(Xtest, Ytest))\n",
    "    print(\"Time to compute test accuracy:\", (datetime.now() - t0), \"Test size:\", len(Ytest))"
   ],
   "id": "2da63a6231571dfc",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
