{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Non-Naive Bayes Classifier",
   "id": "6b11e76104b629f"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Import Libraries:",
   "id": "971b54deb4a75ffc"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-23T23:48:57.395266Z",
     "start_time": "2026-01-23T23:48:47.048123Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "from util import get_data\n",
    "from datetime import datetime\n",
    "from scipy.stats import norm\n",
    "from scipy.stats import multivariate_normal as mvn"
   ],
   "id": "fda638f26fe326e2",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### In the Non-Naive Bayes approach, we do not assume independence between features. Instead, we model the relationships and correlations between them using the full structure of the Multivariate Gaussian distribution.\n",
    "\n",
    "#### The Formula (Dependent Features)\n",
    "\n",
    "When features are dependent, the joint probability $p(x)$ cannot be simplified into a simple product. We must use the general form of the Multivariate Normal distribution:\n",
    "\n",
    "$$p(x | \\mu, \\Sigma) = \\frac{1}{(2\\pi)^{d/2} |\\Sigma|^{1/2}} \\exp\\left( -\\frac{1}{2} (x - \\mu)^T \\Sigma^{-1} (x - \\mu) \\right)$$\n",
    "\n",
    "Where:\n",
    "* $d$ is the number of features.\n",
    "* $\\mu$ is the mean vector for all features.\n",
    "* $\\Sigma$ is the Full Covariance Matrix.\n",
    "* $|\\Sigma|$ is the determinant of the covariance matrix.\n",
    "* $\\Sigma^{-1}$ is the inverse of the covariance matrix.\n",
    "\n",
    "#### Why use a Full Covariance Matrix?\n",
    "\n",
    "In a Non-Naive model, we acknowledge that features often influence one another.\n",
    "1) Non-Zero Covariance: The covariance between different features $i$ and $j$ ($i \\neq j$) can be anything, not just zero.\n",
    "2) Structure of $\\Sigma$: The matrix $\\Sigma$ is a full square matrix where the off-diagonal elements capture the \"non-naive\" dependencies between variables.\n",
    "\n",
    "$$\\Sigma = \\begin{bmatrix} \\sigma_1^2 & \\text{cov}(1, 2) & \\dots & \\text{cov}(1, d) \\\\ \\text{cov}(2, 1) & \\sigma_2^2 & \\dots & \\text{cov}(2, d) \\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\ \\text{cov}(d, 1) & \\text{cov}(d, 2) & \\dots & \\sigma_d^2 \\end{bmatrix}$$"
   ],
   "id": "7c382013045f726b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-23T23:48:57.410430Z",
     "start_time": "2026-01-23T23:48:57.401858Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class NaiveBayes(object):\n",
    "    def fit(self, X, Y, smoothing = 10e-3):\n",
    "        N, D = X.shape\n",
    "\n",
    "        # Creating empty dictionaries\n",
    "        self.gaussians = dict()\n",
    "            # self.gaussians = {0:{\"mean\": 3.41, 4.42, 5.23, \"variance\": 1.34, 1.23, 1.53}, 1:mean\": ..., \"variance\": ...}}\n",
    "\n",
    "        self.priors = dict()\n",
    "            # self.priors = {0:p(Y = 0), 1:p(Y = 1)}\n",
    "\n",
    "        # Unique values of Y\n",
    "        labels = set(Y)\n",
    "\n",
    "        # Filling in data for our empty dictionaries\n",
    "        for c in labels:\n",
    "            current_x = X[Y == c]\n",
    "            self.gaussians[c] = {\"mean\" : current_x.mean(axis = 0), \"covariance\" : np.cov(current_x.T) + np.eye(D)*smoothing}\n",
    "            # P(Y) - Prior\n",
    "            self.priors[c] = float(len(Y[Y == c]) / len(Y))\n",
    "\n",
    "    def predict(self, X):\n",
    "        N, D = X.shape\n",
    "        K = len(self.gaussians)\n",
    "        P = np.zeros((N, K))\n",
    "        for c, g in self.gaussians.iteritems():\n",
    "            mean, variance = g[\"mean\"], g[\"covariance\"]\n",
    "            # P(X|Y) - Likelihood MVN\n",
    "            P[:, c] = mvn.logpdf(X, mean=mean, covariance=covariance) + np.log(self.priors[c])\n",
    "        return np.argmax(P, axis = 1)\n",
    "\n",
    "    def score(self, X, Y):\n",
    "        P = self.predict(X)\n",
    "        return np.mean(P == Y)"
   ],
   "id": "8311b79d32a132bb",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Load data, run all code",
   "id": "6044ba64c5c48f42"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-24T18:12:04.790884Z",
     "start_time": "2026-01-24T18:12:04.786893Z"
    }
   },
   "cell_type": "code",
   "source": [
    "if __name__ == '__main__':\n",
    "    X, Y = get_data(10000)\n",
    "    Ntrain = len(Y) / 2\n",
    "    Xtrain, Ytrain = X[:Ntrain], Y[:Ntrain]\n",
    "    Xtest, Ytest = X[Ntrain:], Y[Ntrain:]\n",
    "\n",
    "    model = NaiveBayes()\n",
    "    t0 = datetime.now()\n",
    "    model.fit(Xtrain, Ytrain)\n",
    "    print(\"Training time: \", datetime.now() - t0)\n",
    "\n",
    "    t0 = datetime.now()\n",
    "    print(\"Test accuracy:\", model.score(Xtest, Ytest))\n",
    "    print(\"Time to compute test accuracy:\", (datetime.now() - t0), \"Test size:\", len(Ytest))"
   ],
   "id": "2da63a6231571dfc",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
